{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RY72tV-4cJpu"
   },
   "source": [
    "# Sentiment Analysis of Bengali Texts\n",
    "We will go through all the steps towards Sentiment Analaysis of Bengali Language with Deep Learning Model. \n",
    "\n",
    "There are very few published works on this topic. Specially when I was working with Sentiment Analysis of Bengali Language for my final year's thesis I didn't find any stuctured work that will lead me from scratch to results. So I am writting the code bellow that will lead from loading dataset to constructing model, to make prediction with the model.\n",
    "\n",
    "Withot further discussion, let's dive into code. I will be writing the code in google colab. So if you run in your local environment make sure you have installed all the packages/modules. And All my files will be uploaded from drive. You can try it with your own dataset if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISaHhbcqGzYq"
   },
   "source": [
    "First, let's Check if the colab GPU is connected, execute following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIIKLzoNb5D_",
    "outputId": "7fecde8f-cabd-48f7-b4c3-0034e32f5853"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c2wkasMfPYL"
   },
   "source": [
    "Let's import neessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0YVoyJ6T54LX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 03:02:15.801615: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-13 03:02:15.801644: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D,Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cdoEsh8fWOb"
   },
   "source": [
    "# BNLP Toolkit\n",
    "\n",
    "Before feeding data to a Neural Network, we need to prepare the data for the neural network. To remove noise from the data, and to make it fit for the neural network, we will go throuth several steps \n",
    "\n",
    "In the first steps of prepocessing, BNLP tool kit will help us. We will use it for POS tagging, punctuation removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "woctkktE-HMa",
    "outputId": "d3bdb22f-d2e9-4ae1-e503-1490b56b170b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bnlp_toolkit in /home/ubuntu/.local/lib/python3.8/site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (from bnlp_toolkit) (1.21.0)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.8/site-packages (from bnlp_toolkit) (1.7.1)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/.local/lib/python3.8/site-packages (from bnlp_toolkit) (3.6.5)\n",
      "Requirement already satisfied: wasabi in /home/ubuntu/.local/lib/python3.8/site-packages (from bnlp_toolkit) (0.8.2)\n",
      "Requirement already satisfied: gensim==4.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from bnlp_toolkit) (4.0.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.8/site-packages (from bnlp_toolkit) (0.1.96)\n",
      "Requirement already satisfied: sklearn-crfsuite in /home/ubuntu/.local/lib/python3.8/site-packages (from bnlp_toolkit) (0.3.6)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from gensim==4.0.1->bnlp_toolkit) (5.2.1)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk->bnlp_toolkit) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk->bnlp_toolkit) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk->bnlp_toolkit) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk->bnlp_toolkit) (2021.11.10)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (1.14.0)\n",
      "Requirement already satisfied: tabulate in /home/ubuntu/.local/lib/python3.8/site-packages (from sklearn-crfsuite->bnlp_toolkit) (0.8.9)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from sklearn-crfsuite->bnlp_toolkit) (0.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install bnlp_toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXgXMrK9ANEu",
    "outputId": "9dd8ee05-2ed9-41cf-eeaf-8b1e5436d188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "2.7.0\n",
      "Python 2.7.18\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)\n",
    "print(tf.__version__)\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmzG5v1lp84S"
   },
   "source": [
    "# Dataset\n",
    "The following blocks will load the dataset and we'll see their shapes.\n",
    "\n",
    "I have used \"Bengali Book Review Dataset\" which was published by E. Hossain and his team. Thanks to them. Go google for it and you will find the dataset. I am not providing the original dataset here for some reason.\n",
    "\n",
    "\n",
    "N.B.: I am loading data from my google drive. Make sure you give appropriate path to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgmuwajE8hcu",
    "outputId": "76acb8ab-2bcf-447c-ed90-a123874f85fe"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16594/1408506528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ZOI6C6R54Lo",
    "outputId": "329894ac-e6d0-493f-a5fb-f562aacd4055"
   },
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_excel(r\"/content/drive/My Drive/Thesis/bookReviews_2000.xlsx\")\n",
    "\n",
    "movie_reviews.isnull().values.any()\n",
    "\n",
    "movie_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "4xD6gFKO54Lu",
    "outputId": "c04039a1-a296-4650-de3a-4b2c9844dfd1"
   },
   "outputs": [],
   "source": [
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWBWCo4654L1"
   },
   "outputs": [],
   "source": [
    "#movie_reviews[\"Text\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "FGgaF64o54L6",
    "outputId": "66b96128-1ac1-46e8-e5b3-ad6b95cd92c8"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns_plot=sns.countplot(x='Sentiment', data=movie_reviews)\n",
    "fig = sns_plot.get_figure()\n",
    "fig.savefig(\"data_shape.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWob8mCe54MG"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    \n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i4dcwD8W54MN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16594/1373861455.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTAG_RE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'<[^>]+>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTAG_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00-DdHk854MR"
   },
   "outputs": [],
   "source": [
    "Z=[]\n",
    "Sent=[]\n",
    "sentence_tokens_list = []\n",
    "word_tokens_list = []\n",
    "pos_tags_list = []\n",
    "sentences = []\n",
    "\n",
    "sentence_tokens_list.clear()\n",
    "word_tokens_list.clear()\n",
    "pos_tags_list.clear()\n",
    "sentences.clear()\n",
    "\n",
    "\n",
    "sentence = list(movie_reviews['Review'])\n",
    "\n",
    "for sen in sentence:\n",
    "    sentences.append(preprocess_text(sen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9iotJG2254MY",
    "outputId": "46c1bc8c-8d20-4739-83db-3ffd4c5782f7"
   },
   "outputs": [],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6XmeeDV54Mc"
   },
   "outputs": [],
   "source": [
    "# X.clear()\n",
    "\n",
    "Z.clear()\n",
    "\n",
    "Sent.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiG0TelSqUZz"
   },
   "source": [
    "# POS tagging and Punctuation Removal\n",
    "\n",
    "In the following code, we will first have word tokens from each sentence with the help of BNLP toolkit. Then with POS tagging, we'll detect punctuations and remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSAvOONf54Mh",
    "outputId": "4d2b2cae-8067-42b9-d56e-6a1fa2c4f158"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7098/402788389.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# X.clear()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlocal_pos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlocal_pos_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# X.clear()\n",
    "Z.clear()\n",
    "local_pos_tags = []\n",
    "local_pos_tags.clear()\n",
    "pos_tags_list.clear()\n",
    "\n",
    "import copy ## to implement shallow copy. See copy in python\n",
    "\n",
    "from bnlp import NLTKTokenizer\n",
    "from bnlp import POS\n",
    "bn_pos = POS()\n",
    "model_path = \"/content/drive/My Drive/Thesis/bn_pos_model.pkl\"\n",
    "for i in range(len(sentences)):\n",
    "    text = sentences[i]\n",
    "    bnltk = NLTKTokenizer()\n",
    "    \n",
    "    \n",
    "    word_tokens = bnltk.word_tokenize(text)\n",
    "    word_tokens_copy = copy.copy(word_tokens)\n",
    "#     word_tokens_list.append(word_tokens)\n",
    "    \n",
    "    \n",
    "    sentence_tokens = bnltk.sentence_tokenize(text)\n",
    "    sentence_tokens_list.append(sentence_tokens)\n",
    "    \n",
    "    for j in range(len(word_tokens)):\n",
    "        word = word_tokens[j]    \n",
    "        pos_tags = bn_pos.tag(model_path, word)\n",
    "        \n",
    "        if(pos_tags[0][1] == \"PU\"):\n",
    "            word_tokens_copy.remove(str(word))\n",
    "\n",
    "            \n",
    "    word_tokens_list.append(word_tokens_copy)\n",
    "\n",
    "            \n",
    "\n",
    "print(\"\\n Sentence Tokens\")\n",
    "print(sentence_tokens_list[:3])\n",
    "\n",
    "print(\"\\n Word Tokens\")\n",
    "print(word_tokens_list[:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl4qm3-f54Mq"
   },
   "outputs": [],
   "source": [
    "## We are creating word tokens first in 'word_tokens' . Then a shallow copy is made of 'word_tokens' to \n",
    "## 'word_tokens_copy'. We then check the POS tag of the tokens and if any one of them is \"PU\" (i.e punctuation)\n",
    "## then we remove it from 'word_tokens_copy'. After iterating through all the tokens (hence removing all the \"PU\")\n",
    "## we add 'word_tokens_copy' to 'word_tokens_list' .\n",
    "\n",
    "## All these mess is done to remove the error 'list index out of range' while removing an element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW_zFznTanUR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-LOTHzv585g"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "copy_of_word_tokens_list = [] ## It is costly to compute \"word_tokens_list\".\n",
    "                            ## So I'm keeping an copy so that I can experiment with one and change it as wish\n",
    "copy_of_word_tokens_list.clear()\n",
    "copy_of_word_tokens_list = copy.copy(word_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mk06ZIdrOW17"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-VC1FndygJz"
   },
   "outputs": [],
   "source": [
    "word_tokens_list = copy.copy(copy_of_word_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mWqZ6mnsmOw"
   },
   "source": [
    "While removing punctations, there were some english sentence and they were removed. So, with the following block, I'm removing those empty arrays. In this way, we have no english sentance in the dataset. The dataset consists of only the Bangla Sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICwV96g054M5",
    "outputId": "07702754-dd7d-477e-d7f0-2f61754443a7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "remove_counter = 0  ## to check if any array is removed for being empty\n",
    "token_lngth = 0\n",
    "token_length = len(word_tokens_list) ## to iterate over the whole \"word_tokens_list\" array\n",
    "empty_positions = [] ## keep track of empty positions\n",
    "empty_positions.clear()\n",
    "print(token_length) ## How many positions are empty actually \n",
    "\n",
    "for i in range(token_length):\n",
    "    if(len(copy_of_word_tokens_list[i]) == 0):\n",
    "        word_tokens_list.pop(i)\n",
    "        empty_positions.append(i)\n",
    "        remove_counter += 1\n",
    "        print(\"a\")\n",
    "        \n",
    "        \n",
    "print(remove_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvtpzV9Z54M0",
    "outputId": "2fceeba5-c2ef-40c0-c1ef-73142a781bae"
   },
   "outputs": [],
   "source": [
    "print(len(word_tokens_list))\n",
    "\n",
    "print(len(sentence_tokens_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdoRzpxdtXRF"
   },
   "source": [
    "# Word2vec\n",
    "\n",
    "Now we will generate our own wordvector model from all the words in our dataset which will be used later to generate embeddding matrix. If are new and don't know about what wordvector and embedding matrix are, please go google it, you will have some great help.\n",
    "\n",
    "\n",
    "To generate wordvector model, we'll use gensim word2vec with skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PESR1sX_54NA"
   },
   "outputs": [],
   "source": [
    "#word2vec\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pZYpItl54NH"
   },
   "outputs": [],
   "source": [
    "model1 =Word2Vec(word_tokens_list, vector_size=100, window=15, min_count=1,sg=0)\n",
    "#model2 =Word2Vec.load('bn_w2v_model.text')\n",
    "model1.wv.save_word2vec_format(\"/content/drive/My Drive/Thesis/myword2vec_model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBeNnT8Lt_qP"
   },
   "source": [
    "Let's see an example how words are stored in wordvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMXTHO6754NN",
    "outputId": "91bf174e-e5cb-47ca-eddb-67f5b5b904f5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model1.wv[\"ভালো\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JR3KcMz54NW",
    "outputId": "bd0e393e-55c6-432d-d3e4-1412ceba2f2f"
   },
   "outputs": [],
   "source": [
    "y = movie_reviews['Sentiment']\n",
    "\n",
    "print(len(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7BsCxy1uInT"
   },
   "source": [
    "In the following block, we'll remove the label of corresponding english sentances from dataset, as we have removed them previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDeZrSbE54Nb",
    "outputId": "dd5c33ca-e5ef-45d7-814a-ef4660670e53"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(empty_positions)):\n",
    "    y.pop(empty_positions[i])\n",
    "\n",
    "print(len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbRqLefVueqw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rEBpS5Judbz"
   },
   "source": [
    "# Train-Test Split\n",
    "\n",
    "now we'll devide the total data into train and test set. I have taken 10% data for test. Feel free to chose the percentage as you choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn3m51kO54Nf",
    "outputId": "ab092e7d-7b4f-4023-bc6d-07871dfa2a4c"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(word_tokens_list, y, test_size=0.10, random_state=42)\n",
    "\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2t3WELIy6ITm",
    "outputId": "347a14ca-5fa1-494e-a2d5-d8122baa6670"
   },
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mih7IJDuxun"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Neural networks can only work with numbers. But all we have are texts. So to feed them into neural-net, we must convert them to numbers. With Tokenizzation we have done it in the following block. Again, if you don't know what tokenization is, go search for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtHWxIG854Nk"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jNxV_E2vQga"
   },
   "source": [
    "Let's see the result of tokenization with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbWaBNCi54Np",
    "outputId": "79eb2293-c364-41a5-cb12-63c35de09550"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EAJfMWdvX4o"
   },
   "source": [
    "# Padding\n",
    "\n",
    "Padding is necessary as input to a neural network is always fixed sized. But the sentences we are working on are of different lengths. With padding, we add extra zeros to make them of the same length. \n",
    "I have used post padding and set pad length to 100.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhRU5i3a54N1"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# print(vocab_size)\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqFLWu8M54N5",
    "outputId": "a1663a27-8edb-4aa8-9212-ca28de1c8284"
   },
   "outputs": [],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDf658Orv1Ct"
   },
   "source": [
    "# Embedding matrix\n",
    "\n",
    "Now we'll generate our embedding matrix. We'll use our previously generated wordvector model to generate the embedding matrix which will later be used in the embedding layer of our Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eT1qK6nY54N-"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('/content/drive/My Drive/Thesis/myword2vec_model.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggDg6oc954OB",
    "outputId": "d9456dfa-0064-4761-c617-a864c11cbf1a"
   },
   "outputs": [],
   "source": [
    "embedding_matrix[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Km2ld-Boz8gV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuVC38XtwaEa"
   },
   "source": [
    "# Neural Network / Deep Learning model.\\\n",
    "\n",
    "All of our prerpocessing tasks are done! Now we'll build our deep learning model which will perform sentiment analysis for us. \n",
    "\n",
    "In my model, I have used a combination of CNN and RNN Layers with the Dense layers. \n",
    "\n",
    "I had done some hyper-parameter tuning for my model. Feel free to use your won model/layers/parameter as you want. This part is totally on you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "o4_mpb3ya9fU",
    "outputId": "e924c8ef-9dd1-4c59-a838-6a9696c851c7"
   },
   "outputs": [],
   "source": [
    "from keras import Sequential, optimizers\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,Bidirectional,ConvLSTM2D,GRU,SimpleRNN,RNN\n",
    "from keras import regularizers\n",
    "\n",
    "model2 = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100,weights=[embedding_matrix], input_length=maxlen , trainable=True)\n",
    "model2.add(embedding_layer)\n",
    "\n",
    "model2.add (Bidirectional(LSTM(64,  return_sequences=True,kernel_regularizer=regularizers.l2(0.15))))\n",
    "\n",
    "model2.add(Conv1D(32, 4, activation='relu'))\n",
    "model2.add (LSTM(32,return_sequences=True, activation = 'tanh') )\n",
    "\n",
    "model2.add(Conv1D(16, 4, activation='relu'))\n",
    "\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "\n",
    "model2.add (Dropout(0.4))\n",
    "\n",
    "model2.add(Dense(16,activation='relu'))\n",
    "\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "#lr=0.0005\n",
    "model2.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znb_nKpoxXtW"
   },
   "source": [
    "Let's see the model's summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVBVUp8cekH-",
    "outputId": "2d9c002d-77c7-4c87-f9ea-79d897bbf174"
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDx_w5w2xbRf"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "And train the model with our training data. Again, feel free to chose the batch_size, epochs etc as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqYXfnCwcHKj",
    "outputId": "b70cf771-8839-43ad-c300-11f9c5af1fb6"
   },
   "outputs": [],
   "source": [
    "history = model2.fit(X_train, y_train, batch_size=32, epochs=70, verbose=1, validation_split=0.10)\n",
    "\n",
    "score = model2.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyBj-RskxtEN"
   },
   "source": [
    "# Plot the model's performance\n",
    "\n",
    "The following block will generate model's accuracy and loss graph with training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IEfI_LTo54OR",
    "outputId": "fb81496c-2c04-4694-9c43-e8d5535d3b6e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16594/185157846.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc = 'upper left')\n",
    "plt.savefig('/content/drive/My Drive/Thesis/model_Accuracy.png',dpi=600)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc = 'upper left')\n",
    "plt.savefig('/content/drive/My Drive/Thesis/model_loss.png', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VTqAZxhyDoX"
   },
   "source": [
    "# Accuracy measures \n",
    "\n",
    "Now we will test our model with the test data that our model has never seen. \n",
    "\n",
    "We will check the accuracy, recall, precision and f-1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHomTwMF54OW",
    "outputId": "5108e784-de7c-4b1b-d68b-34678d91dce9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "yhat_classes = model2.predict_classes(X_test, verbose=1)\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(y_test, yhat_classes)\n",
    "print(matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsYs-7F6wQWz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ERyTqSsyiiy"
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "f-S1REL454Oc",
    "outputId": "ec5e97f2-ab54-4536-f86b-529359e91f2e"
   },
   "outputs": [],
   "source": [
    "# confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, xticklabels='01', yticklabels='01', annot=True, fmt='g')\n",
    "\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('/content/drive/My Drive/Thesis/confussion_matrix.png', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F7FfK6aynqA"
   },
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "QNYG1jMFwALG",
    "outputId": "17e2b48e-15ad-4e4d-fb84-967d42095173"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test,yhat_classes)\n",
    "# plt.figure(figsize=(14,8))\n",
    "plt.plot(fpr, tpr, color=\"red\")\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Roc curve')\n",
    "plt.savefig('/content/drive/My Drive/Thesis/roc_curve.png', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO9mIhvn9UGi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iu8ike-M9T7F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Seniment_analysis.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c14d9d0eeb2f1c6cfd6fcd905562e8d91fc10062278dc2a77fcc9d4f21b13f83"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
