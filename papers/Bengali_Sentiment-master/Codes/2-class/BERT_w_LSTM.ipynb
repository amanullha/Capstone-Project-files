{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "BERT w/ LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7nGmu9p0eQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5yJ8nS9uqXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EtQnb_sxzwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7fFtnmsuqXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj9bZUdruqXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExdPlypHuqX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_input_length = 400\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKxMQPPSuqX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmO_HoSIuqX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "TEXT = data.Field(sequential= True,\n",
        "                  batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(sequential= False,dtype = torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODjHAIrI6yEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data= data.TabularDataset(\n",
        "    path='/content/drive/My Drive/Research_Shanto/Datasets/Ashik Bhai_Sentiment/Code/real_imb_senti_train_dup.csv',  format='CSV',\n",
        "    fields=[('text', TEXT), ('labels', LABEL)])\n",
        "\n",
        "\n",
        "test_data= data.TabularDataset(\n",
        "    path='/content/drive/My Drive/Research_Shanto/Datasets/Ashik Bhai_Sentiment/Code/real_imb_senti_test.csv',  format='CSV',\n",
        "    fields=[('text', TEXT), ('labels', LABEL)])\n",
        "\n",
        "# valid_data= data.TabularDataset(\n",
        "#     path='/content/drive/My Drive/Research_Shanto/Datasets/SemEvel/proc_SemEvel_2016_devtest.csv',  format='CSV',skip_header=True,\n",
        "#     fields=[('text', TEXT), ('labels', LABEL)])\n",
        "train_data, valid_data = train_data.split(split_ratio=0.85,random_state = random.seed(SEED))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rnb_ldOuqYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_VUS6mHQU_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data.weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNuPs4b3QwfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(vars(valid_data.examples[6])['text'])\n",
        "print(tokens)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAJ8LdRDuqYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vars(train_data.examples[6]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urV9k2iuuqYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwK1sgnguqYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JZpyBjeuqYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-WdiHBKB62r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK0a2tLbuqYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "train_iterator,valid_iterator,test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data,test_data), \n",
        "    sort_key=lambda x: len(x.text),\n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deeacDO-uqYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKF1EngTuqYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, bert, batch_size, hidden_dim, num_layers, output_dim, dropout, dropout_emb):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        C = output_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.dropout_embed = nn.Dropout(dropout_emb)\n",
        "        self.bilstm = nn.LSTM(768, self.hidden_dim, num_layers=self.num_layers, bias=True, bidirectional=True,\n",
        "                              dropout=dropout)\n",
        "        \n",
        "        self.hidden2label = nn.Linear(self.hidden_dim * 2, C)\n",
        "        self.hidden = self.init_hidden(self.num_layers, batch_size)\n",
        "    \n",
        "\n",
        "    def init_hidden(self, num_layers, batch_size):\n",
        "        # the first is the hidden h\n",
        "        # the second is the cell  c\n",
        "        return (Variable(torch.zeros(2 * num_layers, batch_size, self.hidden_dim)),\n",
        "                Variable(torch.zeros(2 * num_layers, batch_size, self.hidden_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bert(x)[0]\n",
        "        # print(\"After Bert\")\n",
        "        # x = self.embed(x)\n",
        "        # x = self.dropout_embed(x)\n",
        "        # print(\"After dropout_embed\")\n",
        "        # x = x.view(len(x), x.size(1), -1)\n",
        "        # x = embed.view(len(x), embed.size(1), -1)\n",
        "        bilstm_out, self.hidden = self.bilstm(x )\n",
        "        # print(\"After Bilstm\")\n",
        "        # print(bilstm_out.size())\n",
        "        # print(self.hidden)\n",
        "        # print(bilstm_out.shape)\n",
        "\n",
        "        # bilstm_out = torch.transpose(bilstm_out, 0, 1)\n",
        "        # print(\"After Transpose 1\")\n",
        "        # print(bilstm_out.shape)\n",
        "        bilstm_out = torch.transpose(bilstm_out, 1, 2)\n",
        "        # print(\"After Transpose 2\")\n",
        "        # print(bilstm_out.shape)\n",
        "        bilstm_out = torch.tanh(bilstm_out)\n",
        "        # print(\"After tanh\")\n",
        "        # print(bilstm_out.shape)\n",
        "        bilstm_out = F.max_pool1d(bilstm_out, bilstm_out.size(2)).squeeze(2)\n",
        "        # print(\"After maxpool\")\n",
        "        # print(bilstm_out.shape)\n",
        "        bilstm_out = torch.tanh(bilstm_out)\n",
        "        # print(\"After last tanh\")\n",
        "        \n",
        "        # bilstm_out = self.dropout(bilstm_out)\n",
        "\n",
        "        # bilstm_out = self.hidden2label1(bilstm_out)\n",
        "        # logit = self.hidden2label2(F.tanh(bilstm_out))\n",
        "        # print(bilstm_out.shape)\n",
        "\n",
        "        logit = self.hidden2label(bilstm_out)\n",
        "\n",
        "        return logit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loinaxlFuqYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_DIM = 100\n",
        "OUTPUT_DIM = 2\n",
        "N_LAYERS = 3\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = BiLSTM(bert, BATCH_SIZE, HIDDEN_DIM, N_LAYERS, OUTPUT_DIM, DROPOUT, dropout_emb = 0.5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We0Zo-zpuqYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T16Y1jHmuqYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5hX50KLuqYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyxk1e_uuqYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-a2_rQiuqY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=5e-4, weight_decay=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIbZhRWDuqY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weight = 1./ torch.FloatTensor([ , ])\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4mIImTnuqY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrCehk3xuqY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.max(preds, 1)[1]\n",
        "    # print(\"rounded_preds: \",rounded_preds)\n",
        "    # print(\"y: \",y)\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWiVAvIAuqY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    # hid = model.initialize_hidden_state(BATCH_SIZE, device)\n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "\n",
        "        predictions = model(batch.text)\n",
        "        \n",
        "        loss = criterion(predictions, batch.labels)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.labels)\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkOoMfX3uqZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    all_predictions = []\n",
        "    targs = []\n",
        "    # hid = model.initialize_hidden_state(BATCH_SIZE, device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            # hid= tuple([each.data for each in hid])\n",
        "\n",
        "            predictions = model(batch.text)\n",
        "            rounded_predictions = torch.max(predictions, 1)[1]\n",
        "            # print(\"rounded_predictions: \",rounded_predictions)\n",
        "            all_predictions.extend(rounded_predictions.cpu().detach().numpy())\n",
        "            # print(\"all_predictions: \",all_predictions)\n",
        "\n",
        "            targs.extend(batch.labels.cpu().detach().numpy())\n",
        "\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), all_predictions, targs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_GDmFqduqZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnLW2MJyuqZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 20\n",
        "loss_t = []\n",
        "loss_v = []\n",
        "y_ = []\n",
        "train_ac = []\n",
        "val_ac = []\n",
        "best_acc = float('inf')\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    y_.append(int(epoch+1))\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc ,a,b= evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_acc = valid_acc\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/Research_Shanto/Datasets/Ashik Bhai_Sentiment/Code/Gsuit_BertCNN.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    loss_v.append(valid_loss)\n",
        "    loss_t.append(train_loss)\n",
        "    train_ac.append(train_acc)\n",
        "    val_ac.append(valid_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc6I0S8YuqZL",
        "colab_type": "text"
      },
      "source": [
        "We'll load up the parameters that gave us the best validation loss and try these on the test set - which gives us our best results so far!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLHoLA7VciFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/My Drive/Research_Shanto/Datasets/Ashik Bhai_Sentiment/Code/Gsuit_BertCNN.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b67Rz6PvuqZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc, all_predictions,targs = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}